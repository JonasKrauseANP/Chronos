{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INITIALIZATION #############################\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "############ Libraries ##############\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "\n",
    "epislon = 1e-20  # Define a small epsilon value for division by zero cases\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "  return np.sqrt(mse(y_true, y_pred))\n",
    "\n",
    "def mase(y_true, y_pred, y_baseline):\n",
    "    # Calcula o MAE do modelo\n",
    "    mae_pred = np.mean(np.abs(y_true - y_pred))\n",
    "    # Calcula o MAE do modelo baseline Persistent Window (i.e., últimas h observações antes do teste)\n",
    "    mae_naive = np.mean(np.abs(y_true - y_baseline))\n",
    "    result = mae_pred/mae_naive\n",
    "    return result\n",
    "\n",
    "def pbe(y_true, y_pred):\n",
    "  if np.sum(y_true)!=0:\n",
    "    return 100*(np.sum(y_true - y_pred)/np.sum(y_true))\n",
    "  else:\n",
    "    return 100*(np.sum(y_true - y_pred)/(np.sum(y_true) + epislon))\n",
    "\n",
    "def pocid(y_true, y_pred):\n",
    "  n = len(y_true)\n",
    "  D = [1 if (y_pred[i] - y_pred[i-1]) * (y_true[i] - y_true[i-1]) > 0 else 0 for i in range(1, n)]\n",
    "  POCID = 100 * np.sum(D) / (n-1)\n",
    "  return POCID\n",
    "\n",
    "def mcpm(rmse_result, mape_result, pocid_result):\n",
    "  er_result = 100 - pocid_result\n",
    "\n",
    "  A1 = (rmse_result * mape_result * np.sin((2*np.pi)/3))/2\n",
    "  A2 = (mape_result * er_result * np.sin((2*np.pi)/3))/2\n",
    "  A3 = (er_result * rmse_result * np.sin((2*np.pi)/3))/2\n",
    "  total = A1 + A2 + A3\n",
    "  return total\n",
    "\n",
    "def znorm(x):\n",
    "  if np.std(x) != 0: \n",
    "      x_znorm = (x - np.mean(x)) / np.std(x)\n",
    "  else:\n",
    "      x_znorm = (x - np.mean(x)) / (np.std(x) + epislon)\n",
    "  return x_znorm\n",
    "\n",
    "def znorm_reverse(x, mean_x, std_x):\n",
    "  x_denormalized = (np.array(x) * std_x) + mean_x\n",
    "  return x_denormalized\n",
    "\n",
    "def get_stats_norm(series, horizon, window):\n",
    "  last_subsequence = series[-(horizon+window):-horizon].values\n",
    "  last_mean = np.mean(last_subsequence)\n",
    "  last_std = np.std(last_subsequence)\n",
    "  return last_mean, last_std\n",
    "\n",
    "# Para predição de vendas por UF (mensal), será considerado horizon = 12\n",
    "# Para predição de vendas por município (anual), será considerado horizon = 1\n",
    "def train_test_split(data, horizon):\n",
    "  X = data.iloc[:,:-1] # features\n",
    "  y = data.iloc[:,-1] # target\n",
    "\n",
    "  X_train = X[:-horizon] # features train\n",
    "  X_test =  X[-horizon:] # features test\n",
    "\n",
    "  y_train = y[:-horizon] # target train\n",
    "  y_test = y[-horizon:] # target test\n",
    "  return X_train, X_test, y_train, y_test\n",
    "\n",
    "def recursive_multistep_forecasting(X_test, model, horizon):\n",
    "  # example é composto pelas últimas observações vistas\n",
    "  # na prática, é o pbeprimeiro exemplo do conjunto de teste\n",
    "  example = X_test.iloc[0].values.reshape(1,-1)\n",
    "\n",
    "  preds = []\n",
    "  for i in range(horizon):\n",
    "    pred = model.predict(example)[0]\n",
    "    preds.append(pred)\n",
    "\n",
    "    # Descartar o valor da primeira posição do vetor de características\n",
    "    example = example[:,1:]\n",
    "\n",
    "    # Adicionar o valor predito na última posição do vetor de características\n",
    "    example = np.append(example, pred)\n",
    "    example = example.reshape(1,-1)\n",
    "  return preds\n",
    "\n",
    "def baseline_mean(series, horizon):\n",
    "  # como as séries são normalizadas, esse baseline irá retornar uma reta próxima de zero\n",
    "  pred = np.repeat(np.mean(znorm(series[:-horizon])), horizon)\n",
    "  return pred\n",
    "\n",
    "def baseline_persistent(series, horizon):\n",
    "  return np.repeat(znorm(series[-2*horizon:-horizon]).values[-1], horizon)\n",
    "\n",
    "def baseline_persistent_window(series, horizon):\n",
    "  subsequence = znorm(series[-horizon*2:-horizon]).values\n",
    "  return subsequence\n",
    "\n",
    "def baseline_persistent_windowR(series, horizon):\n",
    "  subsequence2 = series[-horizon*2:-horizon].values\n",
    "  return subsequence2\n",
    "\n",
    "# Em geral, considera-se um tamanho de janela capaz de capturar um ciclo dos dados\n",
    "# Por exemplo, 12 observações no caso dos dados com frequência mensal\n",
    "def rolling_window(series, window):\n",
    "  data = []\n",
    "  for i in range(len(series)-window):\n",
    "    example = znorm(np.array(series[i:i+window+1]))\n",
    "    data.append(example)\n",
    "  df = pd.DataFrame(data)\n",
    "  return df\n",
    "\n",
    "\n",
    "def extract_estado(file_name):\n",
    "    parts = file_name.split('_')\n",
    "    estado = parts[1]\n",
    "    return estado\n",
    "\n",
    "def read_csv_files(folder_path):\n",
    "    estados = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file_name in files:\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', newline='') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                headers = next(reader)\n",
    "                estado = extract_estado(file_name)\n",
    "                estados.append(estado)\n",
    "                estados.sort()\n",
    "    return estados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### ARROW INDIVIDUAL\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import numpy as np\n",
    "from gluonts.dataset.arrow import ArrowWriter\n",
    "\n",
    "\n",
    "def extract_estado(file_name):\n",
    "    parts = file_name.split('_')\n",
    "    estado = parts[1]\n",
    "    return estado\n",
    "\n",
    "def read_csv_files(folder_path):\n",
    "    estados = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file_name in files:\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', newline='') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                headers = next(reader)\n",
    "                estado = extract_estado(file_name)\n",
    "                estados.append(estado)\n",
    "                estados.sort()\n",
    "    return estados\n",
    "\n",
    "def convert_to_arrow(\n",
    "    path: Union[str, Path],\n",
    "    time_series: Union[List[np.ndarray], np.ndarray],\n",
    "    compression: str = \"lz4\",\n",
    "):\n",
    "    assert isinstance(time_series, list) or (\n",
    "        isinstance(time_series, np.ndarray) and\n",
    "        time_series.ndim == 2\n",
    "    )\n",
    "\n",
    "    # Set an arbitrary start time\n",
    "    # start = np.datetime64(\"1990-01\", \"ns\")\n",
    "    start = \"1990-01\"\n",
    "\n",
    "    dataset = [\n",
    "        {\"start\": start, \"target\": ts} for ts in time_series\n",
    "    ]\n",
    "\n",
    "    ArrowWriter(compression=compression).write_to_file(\n",
    "        dataset,\n",
    "        path=path,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    horizon = 12\n",
    "\n",
    "    products = sorted([name for name in os.listdir('../uf/') if os.path.isdir(os.path.join('../uf/', name))])\n",
    "\n",
    "    for ano in range(2024, 2019, -1):\n",
    "        periodo = (2025 - ano) * 12\n",
    "\n",
    "        for product in products:\n",
    "            folder_path = f'../uf/{product}/'\n",
    "            # Read the CSV files and extract estado names\n",
    "            estados = read_csv_files(folder_path)\n",
    "            for estado in estados:\n",
    "\n",
    "                # Generate 20 random time series of length 1024\n",
    "                time_series_t = pd.read_csv(f\"../uf/{product}/mensal_{estado}_{product}.csv\", header=0, sep=\";\")\n",
    "                time_series = time_series_t['m3'].iloc[:-periodo]\n",
    "                time_series = [time_series.to_numpy()]\n",
    "                # Convert to GluonTS arrow format\n",
    "                convert_to_arrow(f\"../{product}_{estado}_{ano}-data.arrow\", time_series=time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ FINE-TUNE\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# Read the paths from list.txt\n",
    "with open('listarrows.txt', 'r') as file:\n",
    "    paths = file.readlines()\n",
    "\n",
    "\n",
    "###### MODELS\n",
    "# chronos-gpt2.yaml\n",
    "# chronos-t5-base.yaml\n",
    "# chronos-t5-large.yaml\n",
    "# chronos-t5-mini.yaml\n",
    "# chronos-t5-small.yaml\n",
    "# chronos-t5-tiny.yaml\n",
    "\n",
    "\n",
    "modelo = 'chronos-gpt2'\n",
    "# Strip any extra spaces or newline characters\n",
    "paths = [path.strip() for path in paths]\n",
    "\n",
    "# Run the command for each path\n",
    "for path in paths:\n",
    "    # Read the yaml configuration file\n",
    "    with open(f'./chronos-forecasting/scripts/training/configs/{modelo}.yaml', 'r') as yaml_file:\n",
    "        yaml_lines = yaml_file.readlines()\n",
    "    \n",
    "    # Modify the second line of the yaml file with the current path\n",
    "    yaml_lines[1] = f\"  - \\\"{path}\\\"\\n\"\n",
    "\n",
    "    # Write the modified yaml file back\n",
    "    with open(f'./chronos-forecasting/scripts/training/configs/{modelo}.yaml', 'w') as yaml_file:\n",
    "        yaml_file.writelines(yaml_lines)\n",
    "    \n",
    "    # Run the torchrun command\n",
    "    command = f\"torchrun --nproc-per-node=2 ./chronos-forecasting/scripts/training/train.py --config ./chronos-forecasting/scripts/training/configs/{modelo}.yaml\"\n",
    "    subprocess.run(command, shell=True)  # This will wait for the command to finish before moving to the next\n",
    "\n",
    "\n",
    "#1 GPU\n",
    "# CUDA_VISIBLE_DEVICES=0 python ./chronos-forecasting/scripts/training/train.py --config ./chronos-forecasting/scripts/training/configs/chronos-gpt2.yaml\n",
    "\n",
    "#Multiplas GPUs\n",
    "# torchrun --nproc-per-node=2 ./chronos-forecasting/scripts/training/train.py --config ./chronos-forecasting/scripts/training/configs/chronos-gpt2.yaml\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhhiiiiiiii\n",
      "All data consolidated into 'consolidated_output.csv'.\n"
     ]
    }
   ],
   "source": [
    "######## TEST INDIV #############\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd  # requires: pip install pandas\n",
    "import torch\n",
    "from chronos import BaseChronosPipeline\n",
    "\n",
    "####### MUDAR PARA CADA ANO!!! ################\n",
    "\n",
    "ano = 2024\n",
    "\n",
    "periodo = (2025-ano)*12\n",
    "i = 0\n",
    "\n",
    "horizon = 12\n",
    "\n",
    "\n",
    "\n",
    "# List of products\n",
    "products = sorted([name for name in os.listdir('../uf/') if os.path.isdir(os.path.join('../uf/', name))])\n",
    "\n",
    "# Read the first model folder from list2024.txt ####### MUDAR PARA CADA ANO!!! ################\n",
    "# with open(f'list{ano}.txt', 'r') as file:\n",
    "with open(f'list24.txt', 'r') as file:\n",
    "    model_folders = [line.strip() for line in file if line.strip()]  # Remove empty lines and strip spaces\n",
    "\n",
    "\n",
    "\n",
    "# Collect all rows for the final CSV\n",
    "all_data = []\n",
    "\n",
    "try:\n",
    "    for product in products:\n",
    "        folder_path = f'../uf/{product}/'\n",
    "        # Read the CSV files and extract estado names\n",
    "        estados = read_csv_files(folder_path)  # Assuming you have this function defined elsewhere\n",
    "        for estado in estados:\n",
    "            df = pd.read_csv(f\"../uf/{product}/mensal_{estado}_{product}.csv\", header=0, sep=\";\")\n",
    "            df2 = df.copy()\n",
    "            df = df.iloc[:-periodo]\n",
    "            if model_folders:\n",
    "                first_model_folder = model_folders[i]\n",
    "\n",
    "                # Load the first model pipeline\n",
    "                version = \"config.json\"\n",
    "                pipeline = BaseChronosPipeline.from_pretrained(\n",
    "                    f\"./output-P-7/{first_model_folder}/checkpoint-final/\",\n",
    "                    device_map=\"cuda\",  # Use \"cpu\" if you don't have a GPU\n",
    "                    torch_dtype=torch.bfloat16,\n",
    "                )\n",
    "\n",
    "\n",
    "            # Predict using the pipeline\n",
    "            quantiles, mean = pipeline.predict_quantiles(\n",
    "                context=torch.tensor(df['m3']),\n",
    "                prediction_length=12,\n",
    "                quantile_levels=[0.1, 0.5, 0.9],\n",
    "            )\n",
    "\n",
    "            low, median, high = quantiles[0, :, 0], quantiles[0, :, 1], quantiles[0, :, 2]\n",
    "\n",
    "            ########################################################################\n",
    "\n",
    "            Valores_Reais = df2['m3'].tail(horizon).reset_index(drop=True)\n",
    "            basepredictions = baseline_persistent_window(df['m3'], horizon)\n",
    "\n",
    "            predictions_df2 = pd.DataFrame({'Predictions': median})\n",
    "\n",
    "            rmse_result2 = rmse(Valores_Reais, predictions_df2['Predictions'])\n",
    "            mape_result2 = mape(Valores_Reais, predictions_df2['Predictions'])\n",
    "            pocid_result2 = pocid(Valores_Reais, predictions_df2['Predictions'])\n",
    "            mcpm_result2 = mcpm(rmse_result2, mape_result2, pocid_result2)\n",
    "            pbe_result2 = pbe(Valores_Reais, predictions_df2['Predictions'])\n",
    "            mase_result2 = mase(Valores_Reais, predictions_df2['Predictions'], basepredictions)\n",
    "            i += 1\n",
    "            # Collect data for final CSV\n",
    "            all_data.append([\n",
    "                product, estado, first_model_folder, 'Chronos', horizon, mape_result2, pocid_result2, pbe_result2, mase_result2,\n",
    "                *predictions_df2['Predictions'].values\n",
    "            ])\n",
    "except IndexError as e:\n",
    "    print('hhhiiiiiiii')\n",
    "\n",
    "# Write all data to a single CSV file\n",
    "with open(f'consolidated_output{ano}.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write header (modify as needed)\n",
    "    writer.writerow([\n",
    "        'Product', 'Estado', 'Model Folder', 'Model', 'Horizon', 'MAPE', 'POCID', 'PBE', 'MASE',\n",
    "        *[f'P_{i+1}' for i in range(horizon)]\n",
    "    ])\n",
    "    writer.writerows(all_data)\n",
    "\n",
    "print(\"All data consolidated into 'consolidated_output.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronos_OK",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
